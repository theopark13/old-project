{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespear_Text_Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEr-TQxUgG4t"
      },
      "source": [
        "use_colab = True\n",
        "assert use_colab in [True, False]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTdskh9mgUEX",
        "outputId": "98eeb4aa-8b5e-496a-e9c2-512dea91e5b1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmn5Q5Y0gVZL"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gStx4Ea6ga2j"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te63_MQ3gcbJ",
        "outputId": "157138a6-d40e-4b6b-9653-1caa56f15cf3"
      },
      "source": [
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Text length: {len(text)}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Text length: 1115394\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdHw2n-rgclW",
        "outputId": "fda5f614-0f8e-4219-8717-21aa1faf393d"
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VTRiTFAgcxj",
        "outputId": "3a71ad3f-acef-4880-f8d7-f09df36fae1d"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'Unique letters: {len(vocab)}')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique letters: 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OygRy9tHgc7Y"
      },
      "source": [
        "# text indexing\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9pRnoBxgdNH",
        "outputId": "242afed3-24b0-4eab-b9f7-f75224165b22"
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nubBpNMKgdZQ",
        "outputId": "ae06efcc-7124-4338-b770-57e716937d83"
      },
      "source": [
        "print(f'{repr(text[:13])} ---- Index ---- > {text_as_int[:13]}')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- Index ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fLTR9cxgdhX",
        "outputId": "3ea5ad0b-c751-4294-f499-2a956fd8fc6b"
      },
      "source": [
        "# max length\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "    print(idx2char[i.numpy()])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RNwPXApgdm3",
        "outputId": "90aeef6d-2a55-41a7-bcae-e943a423343c"
      },
      "source": [
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beUBqVSpgdrC"
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Y_N_kNxgduZ",
        "outputId": "c396e76d-4dae-4707-b7dc-5440e0b7658b"
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "dataset = dataset.shuffle(10000).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqtVnpgAgdyG"
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedding_dim = 256\n",
        "\n",
        "rnn_units = 1024"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvNt3Vy_gd1y"
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEZfBYYagd5W"
      },
      "source": [
        "model = build_model(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units,\n",
        "    batch_size=BATCH_SIZE)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2kkG5yMgd9A",
        "outputId": "c8fd208c-615b-485c-eed4-9f754bdecd51"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMD4F_OTgeAI"
      },
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "example_batch_loss = loss(target_example_batch, example_batch_predictions)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "raVRDOWygeDQ"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "              loss=loss)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hzeCFI-geGn"
      },
      "source": [
        "# the save point\n",
        "if use_colab:\n",
        "    checkpoint_dir ='./drive/My Drive/train_ckpt/text_gen/exp1'\n",
        "    if not os.path.isdir(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "else:\n",
        "    checkpoint_dir = 'text_gen/exp1'\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_dir,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 monitor='loss',\n",
        "                                                 mode='auto',\n",
        "                                                 save_best_only=True,\n",
        "                                                 verbose=1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2o_Wn2Fi8SI"
      },
      "source": [
        "EPOCHS = 20"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36p55-DIgeJP",
        "outputId": "7a46cb78-bab1-4463-9505-cc2d21abb3b2"
      },
      "source": [
        "history = model.fit(dataset, \n",
        "                    epochs=EPOCHS,\n",
        "                    callbacks=[cp_callback])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 2.5526\n",
            "Epoch 00001: loss improved from inf to 2.55262, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 2.5526\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.8687\n",
            "Epoch 00002: loss improved from 2.55262 to 1.86866, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.8687\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.6234\n",
            "Epoch 00003: loss improved from 1.86866 to 1.62340, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 70ms/step - loss: 1.6234\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.4916\n",
            "Epoch 00004: loss improved from 1.62340 to 1.49156, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.4916\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.4117\n",
            "Epoch 00005: loss improved from 1.49156 to 1.41170, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.4117\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.3560\n",
            "Epoch 00006: loss improved from 1.41170 to 1.35602, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 71ms/step - loss: 1.3560\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.3109\n",
            "Epoch 00007: loss improved from 1.35602 to 1.31092, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 12s 72ms/step - loss: 1.3109\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.2712\n",
            "Epoch 00008: loss improved from 1.31092 to 1.27122, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.2712\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.2337\n",
            "Epoch 00009: loss improved from 1.27122 to 1.23371, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.2337\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.1989\n",
            "Epoch 00010: loss improved from 1.23371 to 1.19887, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 73ms/step - loss: 1.1989\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.1622\n",
            "Epoch 00011: loss improved from 1.19887 to 1.16223, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.1622\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.1250\n",
            "Epoch 00012: loss improved from 1.16223 to 1.12499, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.1250\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.0870\n",
            "Epoch 00013: loss improved from 1.12499 to 1.08699, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.0870\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.0457\n",
            "Epoch 00014: loss improved from 1.08699 to 1.04572, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.0457\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 1.0049\n",
            "Epoch 00015: loss improved from 1.04572 to 1.00493, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 1.0049\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.9636\n",
            "Epoch 00016: loss improved from 1.00493 to 0.96361, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 0.9636\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.9209\n",
            "Epoch 00017: loss improved from 0.96361 to 0.92092, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 0.9209\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.8814\n",
            "Epoch 00018: loss improved from 0.92092 to 0.88140, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 0.8814\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.8403\n",
            "Epoch 00019: loss improved from 0.88140 to 0.84026, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 0.8403\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - ETA: 0s - loss: 0.8036\n",
            "Epoch 00020: loss improved from 0.84026 to 0.80358, saving model to ./drive/My Drive/train_ckpt/text_gen/exp1\n",
            "172/172 [==============================] - 13s 74ms/step - loss: 0.8036\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EcM04KUi-UW"
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(checkpoint_dir)\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1XZj5jni-ky"
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 1000\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  # Generates more predictable text if temperature is lower\n",
        "  temperature = 1.0\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mcv-IyRki-tM",
        "outputId": "df60a10a-3797-44a8-c302-5dc8dd922545"
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: ng your cousin, I\n",
            "Should by and by her music was a mighty\n",
            "Waxery and a slain of all that sins and star.\n",
            "\n",
            "Lieven rust unworthy lord?\n",
            "\n",
            "GLOUCESTER:\n",
            "O, no! thought, easide, princely shepheddewits,\n",
            "And time look hearth and dute and word Claudio\n",
            "Lord Hastings' late let this land on more me? that I may nd,\n",
            "That I entreat the winds of me,\n",
            "But sweeter that we look'd it with me?\n",
            "\n",
            "MIRANNA:\n",
            "You will hear you?\n",
            "From queen, thou liest. And he will come to thee,\n",
            "When I call us,\n",
            "As any man with her, and as much less\n",
            "The country of the people queens for thy love again,\n",
            "You head of conscience, insteemance more again\n",
            "To all execution again\n",
            "me?\n",
            "\n",
            "\n",
            "EDWARD:\n",
            "I would thou wilt be heard, or I'll have heart;\n",
            "And therefore cannot he?\n",
            "\n",
            "GREMIO:\n",
            "Take my help. The king, ure not; for he is honour,\n",
            "Out of her good, he kings and heaps and enter.\n",
            "What m\n",
            "Clarence is Baptixtales: all as mine and thee of the\n",
            "sheep-broad; and 'twas in thy just?\n",
            "For battle ripe and point but white\n",
            "Or they dispatch: thou art too much safe,\n",
            "Your\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
